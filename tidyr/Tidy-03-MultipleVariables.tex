%%- https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html

Multiple variables stored in one column

After gathering columns, the key column is sometimes a combination of multiple underlying variable names. This happens in the tb (tuberculosis) dataset, shown below. This dataset comes from the World Health Organisation, and records the counts of confirmed tuberculosis cases by country, year, and demographic group. The demographic groups are broken down by sex (m, f) and age (0-14, 15-25, 25-34, 35-44, 45-54, 55-64, unknown).

tb <- tbl_df(read.csv("tb.csv", stringsAsFactors = FALSE))
tb
#> Source: local data frame [5,769 x 22]
#> 
#>    iso2 year m04 m514 m014 m1524 m2534 m3544 m4554 m5564 m65 mu f04 f514
#> 1    AD 1989  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 2    AD 1990  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 3    AD 1991  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 4    AD 1992  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 5    AD 1993  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 6    AD 1994  NA   NA   NA    NA    NA    NA    NA    NA  NA NA  NA   NA
#> 7    AD 1996  NA   NA    0     0     0     4     1     0   0 NA  NA   NA
#> 8    AD 1997  NA   NA    0     0     1     2     2     1   6 NA  NA   NA
#> 9    AD 1998  NA   NA    0     0     0     1     0     0   0 NA  NA   NA
#> 10   AD 1999  NA   NA    0     0     0     1     1     0   0 NA  NA   NA
#> ..  ...  ... ...  ...  ...   ...   ...   ...   ...   ... ... .. ...  ...
#> Variables not shown: f014 (int), f1524 (int), f2534 (int), f3544 (int),
#>   f4554 (int), f5564 (int), f65 (int), fu (int)
First we gather up the non-variable columns:

tb2 <- tb %>% 
  gather(demo, n, -iso2, -year, na.rm = TRUE)
tb2
#> Source: local data frame [35,750 x 4]
#> 
#>    iso2 year demo n
#> 1    AD 2005  m04 0
#> 2    AD 2006  m04 0
#> 3    AD 2008  m04 0
#> 4    AE 2006  m04 0
#> 5    AE 2007  m04 0
#> 6    AE 2008  m04 0
#> 7    AG 2007  m04 0
#> 8    AL 2005  m04 0
#> 9    AL 2006  m04 1
#> 10   AL 2007  m04 0
#> ..  ...  ...  ... .
Column headers in this format are often separated by a non-alphanumeric character (e.g. ., -, _, :), or have a fixed width format, like in this dataset. separate() makes it easy to split a compound variables into individual variables. You can either pass it a regular expression to split on (the default is to split on non-alphanumeric columns), or a vector of character positions. In this case we want to split after the first character:

tb3 <- tb2 %>% 
  separate(demo, c("sex", "age"), 1)
tb3
#> Source: local data frame [35,750 x 5]
#> 
#>    iso2 year sex age n
#> 1    AD 2005   m  04 0
#> 2    AD 2006   m  04 0
#> 3    AD 2008   m  04 0
#> 4    AE 2006   m  04 0
#> 5    AE 2007   m  04 0
#> 6    AE 2008   m  04 0
#> 7    AG 2007   m  04 0
#> 8    AL 2005   m  04 0
#> 9    AL 2006   m  04 1
#> 10   AL 2007   m  04 0
#> ..  ...  ... ... ... .
Storing the values in this form resolves a problem in the original data. We want to compare rates, not counts, which means we need to know the population. In the original format, there is no easy way to add a population variable. It has to be stored in a separate table, which makes it hard to correctly match populations to counts. In tidy form, adding variables for population and rate is easy because they're just additional columns.

Variables are stored in both rows and columns

The most complicated form of messy data occurs when variables are stored in both rows and columns. The code below loads daily weather data from the Global Historical Climatology Network for one weather station (MX17004) in Mexico for five months in 2010.

weather <- tbl_df(read.csv("weather.csv", stringsAsFactors = FALSE))
weather
#> Source: local data frame [22 x 35]
#> 
#>         id year month element d1   d2   d3 d4   d5 d6 d7 d8 d9  d10  d11
#> 1  MX17004 2010     1    tmax NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> 2  MX17004 2010     1    tmin NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> 3  MX17004 2010     2    tmax NA 27.3 24.1 NA   NA NA NA NA NA   NA 29.7
#> 4  MX17004 2010     2    tmin NA 14.4 14.4 NA   NA NA NA NA NA   NA 13.4
#> 5  MX17004 2010     3    tmax NA   NA   NA NA 32.1 NA NA NA NA 34.5   NA
#> 6  MX17004 2010     3    tmin NA   NA   NA NA 14.2 NA NA NA NA 16.8   NA
#> 7  MX17004 2010     4    tmax NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> 8  MX17004 2010     4    tmin NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> 9  MX17004 2010     5    tmax NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> 10 MX17004 2010     5    tmin NA   NA   NA NA   NA NA NA NA NA   NA   NA
#> ..     ...  ...   ...     ... ..  ...  ... ..  ... .. .. .. ..  ...  ...
#> Variables not shown: d12 (lgl), d13 (dbl), d14 (dbl), d15 (dbl), d16
#>   (dbl), d17 (dbl), d18 (lgl), d19 (lgl), d20 (lgl), d21 (lgl), d22 (lgl),
#>   d23 (dbl), d24 (lgl), d25 (dbl), d26 (dbl), d27 (dbl), d28 (dbl), d29
#>   (dbl), d30 (dbl), d31 (dbl)
It has variables in individual columns (id, year, month), spread across columns (day, d1-d31) and across rows (tmin, tmax) (minimum and maximum temperature). Months with less than 31 days have structural missing values for the last day(s) of the month.

To tidy this dataset we first gather the day columns:

weather2 <- weather %>%
  gather(day, value, d1:d31, na.rm = TRUE)
weather2
#> Source: local data frame [66 x 6]
#> 
#>         id year month element day value
#> 1  MX17004 2010    12    tmax  d1  29.9
#> 2  MX17004 2010    12    tmin  d1  13.8
#> 3  MX17004 2010     2    tmax  d2  27.3
#> 4  MX17004 2010     2    tmin  d2  14.4
#> 5  MX17004 2010    11    tmax  d2  31.3
#> 6  MX17004 2010    11    tmin  d2  16.3
#> 7  MX17004 2010     2    tmax  d3  24.1
#> 8  MX17004 2010     2    tmin  d3  14.4
#> 9  MX17004 2010     7    tmax  d3  28.6
#> 10 MX17004 2010     7    tmin  d3  17.5
#> ..     ...  ...   ...     ... ...   ...
For presentation, I've dropped the missing values, making them implicit rather than explicit. This is ok because we know how many days are in each month and can easily reconstruct the explicit missing values.

We'll also do a little cleaning:

\begin{framed}
\begin{verbatim}
weather3 <- weather2 %>% 
  mutate(day = extract_numeric(day)) %>%
  select(id, year, month, day, element, value) %>%
  arrange(id, year, month, day)
weather3
#> Source: local data frame [66 x 6]
#> 
#>         id year month day element value
#> 1  MX17004 2010     1  30    tmax  27.8
#> 2  MX17004 2010     1  30    tmin  14.5
#> 3  MX17004 2010     2   2    tmax  27.3
#> 4  MX17004 2010     2   2    tmin  14.4
#> 5  MX17004 2010     2   3    tmax  24.1
#> 6  MX17004 2010     2   3    tmin  14.4
#> 7  MX17004 2010     2  11    tmax  29.7
#> 8  MX17004 2010     2  11    tmin  13.4
#> 9  MX17004 2010     2  23    tmax  29.9
#> 10 MX17004 2010     2  23    tmin  10.7
#> ..     ...  ...   ... ...     ...   ...
This dataset is mostly tidy, but the element column is not a variable; it stores the names of variables. (Not shown in this example are the other meteorological variables prcp (precipitation) and snow (snowfall)). Fixing this requires the spread operation. This performs the inverse of gathering by spreading the element and value columns back out into the columns:

weather3 %>% spread(element, value)
#> Source: local data frame [33 x 6]
#> 
#>         id year month day tmax tmin
#> 1  MX17004 2010     1  30 27.8 14.5
#> 2  MX17004 2010     2   2 27.3 14.4
#> 3  MX17004 2010     2   3 24.1 14.4
#> 4  MX17004 2010     2  11 29.7 13.4
#> 5  MX17004 2010     2  23 29.9 10.7
#> 6  MX17004 2010     3   5 32.1 14.2
#> 7  MX17004 2010     3  10 34.5 16.8
#> 8  MX17004 2010     3  16 31.1 17.6
#> 9  MX17004 2010     4  27 36.3 16.7
#> 10 MX17004 2010     5  27 33.2 18.2
#> ..     ...  ...   ... ...  ...  ...
This form is tidy: there's one variable in each column, and each row represents one day.

Multiple types in one table {#multiple-types}

Datasets often involve values collected at multiple levels, on different types of observational units. During tidying, each type of observational unit should be stored in its own table. This is closely related to the idea of database normalisation, where each fact is expressed in only one place. It's important because otherwise inconsistencies can arise.

The billboard dataset actually contains observations on two types of observational units: the song and its rank in each week. This manifests itself through the duplication of facts about the song: artist, year and time are repeated many times.

This dataset needs to be broken down into two pieces: a song dataset which stores artist, song name and time, and a ranking dataset which gives the rank of the song in each week. We first extract a song dataset:
\begin{framed}
\begin{verbatim}
song <- billboard3 %>% 
  select(artist, track, year, time) %>%
  unique() %>%
  mutate(song_id = row_number())
song
#> Source: local data frame [317 x 5]
#> 
#>            artist                   track year time song_id
#> 1           2 Pac Baby Don't Cry (Keep... 2000 4:22       1
#> 2         2Ge+her The Hardest Part Of ... 2000 3:15       2
#> 3    3 Doors Down              Kryptonite 2000 3:53       3
#> 4    3 Doors Down                   Loser 2000 4:24       4
#> 5        504 Boyz           Wobble Wobble 2000 3:35       5
#> 6            98^0 Give Me Just One Nig... 2000 3:24       6
#> 7         A*Teens           Dancing Queen 2000 3:44       7
#> 8         Aaliyah           I Don't Wanna 2000 4:15       8
#> 9         Aaliyah               Try Again 2000 4:03       9
#> 10 Adams, Yolanda           Open My Heart 2000 5:30      10
#> ..            ...                     ...  ...  ...     ...
Then use that to make a rank dataset by replacing repeated song facts with a pointer to song details (a unique song id):

rank <- billboard3 %>%
  left_join(song, c("artist", "track", "year", "time")) %>%
  select(song_id, date, week, rank) %>%
  arrange(song_id, date)
rank
#> Source: local data frame [5,307 x 4]
#> 
#>    song_id       date week rank
#> 1        1 2000-02-26    1   87
#> 2        1 2000-03-04    2   82
#> 3        1 2000-03-11    3   72
#> 4        1 2000-03-18    4   77
#> 5        1 2000-03-25    5   87
#> 6        1 2000-04-01    6   94
#> 7        1 2000-04-08    7   99
#> 8        2 2000-09-02    1   91
#> 9        2 2000-09-09    2   87
#> 10       2 2000-09-16    3   92
#> ..     ...        ...  ...  ...
You could also imagine a week dataset which would record background information about the week, maybe the total number of songs sold or similar “demographic” information.

Normalisation is useful for tidying and eliminating inconsistencies. However, there are few data analysis tools that work directly with relational data, so analysis usually also requires denormalisation or the merging the datasets back into one table.

One type in multiple tables

It's also common to find data values about a single type of observational unit spread out over multiple tables or files. These tables and files are often split up by another variable, so that each represents a single year, person, or location. As long as the format for individual records is consistent, this is an easy problem to fix:

Read the files into a list of tables.

For each table, add a new column that records the original file name (the file name is often the value of an important variable).

Combine all tables into a single table.

Plyr makes this straightforward in R. The following code generates a vector of file names in a directory (data/) which match a regular expression (ends in .csv). Next we name each element of the vector with the name of the file. We do this because will preserve the names in the following step, ensuring that each row in the final data frame is labeled with its source. Finally, ldply() loops over each path, reading in the csv file and combining the results into a single data frame.
\begin{framed}
\begin{verbatim}
library(plyr)
paths <- dir("data", pattern = "\\.csv$", full.names = TRUE)
names(paths) <- basename(paths)
ldply(paths, read.csv, stringsAsFactors = FALSE)
\end{verbatim}
\end{framed}
Once you have a single table, you can perform additional tidying as needed. An example of this type of cleaning can be found at https://github.com/hadley/data-baby-names which takes 129 yearly baby name tables provided by the US Social Security Administration and combines them into a single file.

A more complicated situation occurs when the dataset structure changes over time. For example, the datasets may contain different variables, the same variables with different names, different file formats, or different conventions for missing values. This may require you to tidy each file to individually (or, if you're lucky, in small groups) and then combine them once tidied. An example of this type of tidying is illustrated in https://github.com/hadley/data-fuel-economy, which shows the tidying of epa fuel economy data for over 50,000 cars from 1978 to 2008. The raw data is available online, but each year is stored in a separate file and there are four major formats with many minor variations, making tidying this dataset a considerable challenge.
